{"cells":[{"cell_type":"markdown","metadata":{"id":"53lY_6Giu_7o"},"source":["# 1. MNIST Dataset\n","\n","We will perform all experiments for this problem using the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a standard dataset of handwritten digits. The main benefits of this dataset are that it is small and relatively easy to model. It therefore allows for quick experimentation and serves as initial test bed in many papers.\n","\n","Another benefit is that it is so widely used that PyTorch even provides functionality to automatically download it.\n","\n","Let's start by downloading the data and visualizing some samples."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"QEq5AXeVlQSC"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":476,"referenced_widgets":["25c51dbd460048a0ade87d231b919eba","2cb89c5dca2444aa899dc73dc3e281f5","e117f794f56b4c45894647d748a0f6f4","61ede69ade0243fb8b9fbcdf8ccabf61","ec99aa403334405184322466cdef6ded","074608e2e6c646d8be29bee287d4eecd","e41aa50f2c594af084fa1af6b9d9a4b3","dd354536c91840b8af906237e2ad1540","ebf2a93e8e29426c9a944ff9f6e53a2a","d6abdefd833a4850a6d4fc5532a0a31f","ec1a9588948e46bb85f9a2cfc2afe690","4c207577fbf14d098de29220c1a200d9","d09b2b8b271b46788d00e6a1442699de","f321c1db62ad4645916b08460fd6b40a","08d643e9e1ed4f5f8a4b938513eb74b2","1069f776c52b4b53be617bbf368fcc5e","5795e5119e744f3b95af1061b694019d","e777a7f4889840be88dffb206d851bf4","430e2502449c47a5a23f47eaf088a894","316e858872234de9a3300a355a32df5d","f8f09be5e62e446cb81ea9ba618a2b05","cc44f8e2f9d94dac82132168dfa615cb","5d868cebd70a4bcf9377ffb62f9adb86","16c9106fb10d43c28815c73e0a960c32","d885bf767bc64103a16be48fce9cb9a9","7aab706d77ee449881a0a1e199a8e845","251a7d7cccc243a7a9afb379f40b6abc","5dbc78cf70d64fc094c758bc5c06b8bb","d80c21ce156748ddae0ff0bcccaccf41","a11343c41bcd4a71912a80830d5bd5b6","f5ee2f461d0c41e8b03f2450ccb0845b","00ccb5258094474484c970e4b698d0e9","669a51ec535b4841b19bafc1cf734534","f39a85e3cf3f4fdfb8141c16c90f5cee","d4caee53c3e049c7ba72e00ba03d358a","d1396522b53047149210c329510307fa","53b496dc0326451994983d34e32eabcf","dece586b88a74ffbb5dba17157cac40c","55b2698f8bab4a2e9b40bad1ac28580c","eee3ae88d3db4a05b23b7f6e14e176cb","0c837a309a8749e9acb998d26b53eb03","79a7c29698064a238d27747ed85b3442","c84c00ce102843c1a5567338fa433747","c1e61faf117747e49ba18e9e8c07c359"]},"id":"Rdkn5v6WvNAa","outputId":"e01140a7-6686-4c94-ed21-0117c997f89f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","\n"," Download complete! Downloaded 60000 training examples!\n"]}],"source":["import torch\n","import torchvision\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')   # use GPU if available\n","print(f\"Using device: {device}\")\n","\n","# this will automatically download the MNIST training set\n","mnist_train = torchvision.datasets.MNIST(root='./data', \n","                                         train=True, \n","                                         download=True, \n","                                         transform=torchvision.transforms.ToTensor())\n","print(\"\\n Download complete! Downloaded {} training examples!\".format(len(mnist_train)))"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gVr1sOuWwk_F"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x5000 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from numpy.random.mtrand import sample\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Let's display some of the training samples.\n","sample_images = []\n","randomize = False # set to False for debugging\n","num_samples = 5 # simple data sampling for now, later we will use proper DataLoader\n","if randomize:\n","  sample_idxs = np.random.randint(low=0,high=len(mnist_train), size=num_samples)\n","else:\n","  sample_idxs = list(range(num_samples))\n","\n","for idx in sample_idxs:\n","  sample = mnist_train[idx]\n","  # print(f\"Tensor w/ shape {sample[0][0].detach().cpu().numpy().shape} and label {sample[1]}\")\n","  sample_images.append(sample[0][0].data.cpu().numpy())\n","  # print(sample_images[0]) # Values are in [0, 1]\n","\n","fig = plt.figure(figsize = (10, 50))   \n","ax1 = plt.subplot(111)\n","ax1.imshow(np.concatenate(sample_images, axis=1), cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"suPd0gyiuvvo"},"source":["# 2. Auto-Encoder\n","\n","Before implementing the full VAE, we will first implement an **auto-encoder architecture**. Auto-encoders feature the same encoder-decoder architecture as VAEs and therefore also learn a low-dimensional representation of the input data without supervision. In contrast to VAEs they are **fully deterministic** models and do not employ variational inference for optimization.\n","\n","The **architecture** is very simple: we will encode the input image into a low-dimensional representation using fully connected layers for the encoder. This results in a low-dimensional representation of the input image. This representation will get decoded back into the dimensionality of the input image using a decoder network that mirrors the architecture of the encoder. The whole model is trained by **minimizing a reconstruction loss** between the input and the decoded image.\n","\n","Intuitively, the **auto-encoder needs to compress the information contained in the input image** into a much lower dimensional representation (e.g. 28x28=784px vs. nz embedding dimensions for our MNIST model). This is possible since the information captured in the pixels is *highly redundant*. E.g. encoding an MNIST image requires <4 bits to encode which of the 10 possible digits is displayed and a few additional bits to capture information about shape and orientation. This is much less than the $255^{28\\cdot 28}$ bits of information that could be theoretically captured in the input image.\n","\n","Learning such a **compressed representation can make downstream task learning easier**. For example, learning to add two numbers based on the inferred digits is much easier than performing the task based on two piles of pixel values that depict the digits.\n","\n","In the following, we will first define the architecture of encoder and decoder and then train the auto-encoder model."]},{"cell_type":"markdown","metadata":{"id":"akSPKMJ0l3rD"},"source":["## Defining the Auto-Encoder Architecture [6pt]"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"rteiFTqfuvTu"},"outputs":[],"source":["import torch.nn as nn\n","\n","# Prob1-1: Let's define encoder and decoder networks\n","class Encoder(nn.Module):\n","  def __init__(self, nz, input_size):\n","    super().__init__()\n","    self.input_size = input_size\n","    ################################# TODO #########################################\n","    # Create the network architecture using a nn.Sequential module wrapper.        #\n","    # Encoder Architecture:                                                        #\n","    # - input_size -> 256                                                          #\n","    # - ReLU                                                                       #\n","    # - 256 -> 64                                                                  #\n","    # - ReLU                                                                       #\n","    # - 64 -> nz                                                                   #\n","    # HINT: Verify the shapes of intermediate layers by running partial networks   #\n","    #        (with the next notebook cell) and visualizing the output shapes.      #\n","    ################################################################################\n","    self.net = nn.Sequential(\n","      nn.Linear(self.input_size, 256),\n","      nn.ReLU(),\n","      nn.Linear(256, 64),\n","      nn.ReLU(),\n","      nn.Linear(64, nz),\n","      )\n","  \n","   \n","    ################################ END TODO #######################################\n","  \n","  def forward(self, x):\n","    return self.net(x)\n","\n","\n","class Decoder(nn.Module):\n","  def __init__(self, nz, output_size):\n","    super().__init__()\n","    self.output_size = output_size\n","    ################################# TODO #########################################\n","    # Create the network architecture using a nn.Sequential module wrapper.        #\n","    # Decoder Architecture (mirrors encoder architecture):                         #\n","    # - nz -> 64                                                                   #\n","    # - ReLU                                                                       #\n","    # - 64 -> 256                                                                  #\n","    # - ReLU                                                                       #\n","    # - 256 -> output_size                                                         #\n","    ################################################################################\n","    self.net = nn.Sequential( \n","      nn.Linear(nz, 64),\n","      nn.ReLU(),\n","      nn.Linear(64, 256),\n","      nn.ReLU(),\n","      nn.Linear(256, self.output_size)\n","    )\n","    \n","    ################################ END TODO #######################################\n","  \n","  def forward(self, z):\n","    return self.net(z).reshape(-1, 1, self.output_size)"]},{"cell_type":"markdown","metadata":{"id":"x0PKgbP4l-n8"},"source":["## Testing the Auto-Encoder Forward Pass"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"-m9ur743t-22"},"outputs":[{"name":"stdout","output_type":"stream","text":["sample_img.shape=torch.Size([64, 1, 28, 28]), <class 'torch.Tensor'>, input.shape=torch.Size([64, 784])\n","Shape of encoding vector (should be [batch_size, nz]): torch.Size([64, 32])\n","Shape of decoded image (should be [batch_size, 1, out_size]): torch.Size([64, 1, 784]).\n"]}],"source":["# To test your encoder/decoder, let's encode/decode some sample images\n","# first, make a PyTorch DataLoader object to sample data batches\n","batch_size = 64\n","nworkers = 2        # number of workers used for efficient data loading\n","\n","####################################################################################\n","# Create a PyTorch DataLoader object for efficiently generating training batches.  #\n","# Make sure that the data loader automatically shuffles the training dataset.      #\n","# Consider only *full* batches of data, to avoid torch errrors.              #\n","# The DataLoader wraps the MNIST dataset class we created earlier.           #\n","#       Use the given batch_size and number of data loading workers when creating  #\n","#       the DataLoader. https://pytorch.org/docs/stable/data.html                  #\n","####################################################################################\n","mnist_data_loader = torch.utils.data.DataLoader(mnist_train, \n","                                                batch_size=batch_size, \n","                                                shuffle=True, \n","                                                num_workers=nworkers,\n","                                                drop_last=True)\n","####################################################################################\n","\n","# now we can run a forward pass for encoder and decoder and check the produced shapes\n","in_size = out_size = 28*28 # image size\n","nz = 32          # dimensionality of the learned embedding\n","encoder = Encoder(nz=nz, input_size=in_size)\n","decoder = Decoder(nz=nz, output_size=out_size)\n","for sample_img, sample_label in mnist_data_loader: # loads a batch of data\n","  input = sample_img.reshape([batch_size, in_size])\n","  print(f'{sample_img.shape=}, {type(sample_img)}, {input.shape=}')\n","  enc = encoder(input)\n","  print(f\"Shape of encoding vector (should be [batch_size, nz]): {enc.shape}\")\n","  dec = decoder(enc)\n","  print(\"Shape of decoded image (should be [batch_size, 1, out_size]): {}.\".format(dec.shape))    \n","  break\n","\n","del input, enc, dec, encoder, decoder, nworkers # remove to avoid confusion later"]},{"cell_type":"markdown","metadata":{"id":"8ssk-NnNE9vO"},"source":["Now that we defined encoder and decoder network our architecture is nearly complete. However, before we start training, we can wrap encoder and decoder into an auto-encoder class for easier handling."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"LziabE_5E9IG"},"outputs":[],"source":["class AutoEncoder(nn.Module):\n","  def __init__(self, nz):\n","    super().__init__()\n","    self.encoder = Encoder(nz=nz, input_size=in_size)\n","    self.decoder = Decoder(nz=nz, output_size=out_size)\n","\n","  def forward(self, x):\n","    enc = self.encoder(x)\n","    return self.decoder(enc)\n","\n","  def reconstruct(self, x):\n","    \"\"\"Only used later for visualization.\"\"\"\n","    enc = self.encoder(x)\n","    flattened = self.decoder(enc)\n","    image = flattened.reshape(-1, 28, 28)\n","    return image"]},{"cell_type":"markdown","metadata":{"id":"wo2rQhAVEYp3"},"source":["## Setting up the Auto-Encoder Training Loop [6pt]\n","After implementing the network architecture, we can now set up the training loop and run training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vWaTyoqEnWK"},"outputs":[],"source":["# Prob1-2\n","epochs = 10\n","learning_rate = 1e-3\n","\n","# build AE model\n","print(f'Device available {device}')\n","ae_model = AutoEncoder(nz).to(device)    # transfer model to GPU if available\n","ae_model = ae_model.train()   # set model in train mode (eg batchnorm params get updated)\n","\n","# build optimizer and loss function\n","####################################### TODO #######################################\n","# Build the optimizer and loss classes. For the loss you can use a loss layer      #\n","# from the torch.nn package. We recommend binary cross entropy.                    #\n","# HINT: We will use the Adam optimizer (learning rate given above, otherwise       #\n","#       default parameters).                                                       #\n","# NOTE: We could also use alternative losses like MSE and cross entropy, depending #\n","#       on the assumptions we are making about the output distribution.            #\n","####################################################################################\n","class Loss(torch.nn):\n","  def __init__(self) -> None:\n","    super().__init__()\n","\n","  def loss_foward(self, y, y_hat):\n","     return nn.BCELoss()\n","\n","class AdamMomentum(torch.optim.Optimizer) : \n","  def __init__(self, lr = 0.001, momentum = 0.9) -> None:\n","    super().__init__()\n","    self.lr = lr\n","    self.momentum = momentum\n","\n","  def step(self, closure=None):\n","    for group in self.param_groups:\n","        for p in group['params']:\n","            if p.grad is None:\n","                continue\n","            grad = p.grad.data\n","            state = self.state[p]\n","\n","            # Initialize momentum\n","            if 'momentum_buffer' not in state:\n","                state['momentum_buffer'] = torch.zeros_like(p.data)\n","\n","            # Apply momentum\n","            momentum = state['momentum_buffer']\n","            momentum.mul_(self.momentum).add_(grad)\n","\n","            # Update parameters\n","            p.data.add_(-self.lr, momentum)\n","    \n","\n","#################################### END TODO #######################################\n","\n","train_it = 0\n","for ep in range(epochs):\n","  print(\"Run Epoch {}\".format(ep))\n","  ####################################### TODO #######################################\n","  # Implement the main training loop for the auto-encoder model.                     #\n","  # HINT: Your training loop should sample batches from the data loader, run the     #\n","  #       forward pass of the AE, compute the loss, perform the backward pass and    #\n","  #       perform one gradient step with the optimizer.                              #\n","  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n","  ####################################################################################\n","  for ...:\n","\n","    if train_it % 100 == 0:\n","      print(\"It {}: Reconstruction Loss: {}\".format(train_it, rec_loss))\n","    train_it += 1\n","  #################################### END TODO #####################################\n","\n","print(\"Done!\")\n","del epochs, learning_rate, sample_img, train_it, rec_loss #, opt"]},{"cell_type":"markdown","metadata":{"id":"3blvOcuUnKpp"},"source":["## Verifying reconstructions\n","Now that we trained the auto-encoder we can visualize some of the reconstructions on the test set to verify that it is converged and did not overfit. **Before continuing, make sure that your auto-encoder is able to reconstruct these samples near-perfectly.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmbdWzLxHuXV"},"outputs":[],"source":["# visualize test data reconstructions\n","def vis_reconstruction(model, randomize=False):\n","  # download MNIST test set + build Dataset object\n","  mnist_test = torchvision.datasets.MNIST(root='./data', \n","                                          train=False, \n","                                          download=True, \n","                                          transform=torchvision.transforms.ToTensor())\n","  model.eval()      # set model in evalidation mode (eg freeze batchnorm params)\n","  num_samples = 5\n","  if randomize:\n","    sample_idxs = np.random.randint(low=0,high=len(mnist_test), size=num_samples)\n","  else:\n","    sample_idxs = list(range(num_samples))\n","\n","  input_imgs, test_reconstructions = [], []\n","  for idx in sample_idxs:\n","    sample = mnist_test[idx]\n","    input_img = np.asarray(sample[0])\n","    input_flat = input_img.reshape(784)\n","    reconstruction = model.reconstruct(torch.tensor(input_flat, device=device))\n","    \n","    input_imgs.append(input_img[0])\n","    test_reconstructions.append(reconstruction[0].data.cpu().numpy())\n","    # print(f'{input_img[0].shape=}\\t{reconstruction.shape}')\n","\n","  fig = plt.figure(figsize = (20, 50))   \n","  ax1 = plt.subplot(111)\n","  ax1.imshow(np.concatenate([np.concatenate(input_imgs, axis=1),\n","                            np.concatenate(test_reconstructions, axis=1)], axis=0), cmap='gray')\n","  plt.show()\n","\n","vis_reconstruction(ae_model, randomize=False) # set randomize to False for debugging"]},{"cell_type":"markdown","metadata":{"id":"i7UO6153nWmC"},"source":["## Sampling from the Auto-Encoder [2pt]\n","\n","To test whether the auto-encoder is useful as a generative model, we can use it like any other generative model: draw embedding samples from a prior distribution and decode them through the decoder network. We will choose a unit Gaussian prior to allow for easy comparison to the VAE later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsWgAMfEn3Qk"},"outputs":[],"source":["# we will sample N embeddings, then decode and visualize them\n","def vis_samples(model):\n","  ####################################### TODO #######################################\n","  # Prob1-3 Sample embeddings from a diagonal unit Gaussian distribution and decode them     #\n","  # using the model.                                                                 #\n","  # HINT: The sampled embeddings should have shape [batch_size, nz]. Diagonal unit   #\n","  #       Gaussians have mean 0 and a covariance matrix with ones on the diagonal    #\n","  #       and zeros everywhere else.                                                 #\n","  # HINT: If you are unsure whether you sampled the correct distribution, you can    #\n","  #       sample a large batch and compute the empirical mean and variance using the #\n","  #       .mean() and .var() functions.                                              #\n","  # HINT: You can directly use model.decoder() to decode the samples.                #\n","  ####################################################################################\n","\n","  #################################### END TODO ######################################\n","\n","  fig = plt.figure(figsize = (10, 10))   \n","  ax1 = plt.subplot(111)\n","  ax1.imshow(torchvision.utils.make_grid(decoded_samples[:16], nrow=4, pad_value=1.)\\\n","                .data.cpu().numpy().transpose(1, 2, 0), cmap='gray')\n","  plt.show()\n","\n","vis_samples(ae_model)\n"]},{"cell_type":"markdown","metadata":{"id":"ExC5BXn3rbap"},"source":[">**Prob1-3 continued: Inline Question: Describe your observations, why do you think they occur? [2pt]** \n",">(max 150 words)\n",">\n",">**Answer:** \n","\n"]},{"cell_type":"markdown","metadata":{"id":"oSUezleArhCI"},"source":["# 3. Variational Auto-Encoder (VAE)\n","\n","Variational auto-encoders use a very similar architecture to deterministic auto-encoders, but are inherently storchastic models, i.e. we perform a stochastic sampling operation during the forward pass, leading to different different outputs every time we run the network for the same input. This sampling is required to optimize the VAE objective also known as the evidence lower bound (ELBO):\n","\n","$$\n","p(x) > \\underbrace{\\mathbb{E}_{z\\sim q(z\\vert x)} p(x \\vert z)}_{\\text{reconstruction}} - \\underbrace{D_{\\text{KL}}\\big(q(z \\vert x), p(z)\\big)}_{\\text{prior divergence}}\n","$$\n","\n","Here, $D_{\\text{KL}}(q, p)$ denotes the Kullback-Leibler (KL) divergence between the posterior distribution $q(z \\vert x)$, i.e. the output of our encoder, and $p(z)$, the prior over the embedding variable $z$, which we can choose freely.\n","\n","For simplicity, we will choose a unit Gaussian prior again. The first term is the reconstruction term we already know from training the auto-encoder. When assuming a Gaussian output distribution for both encoder $q(z \\vert x)$ and decoder $p(x \\vert z)$ the objective reduces to:\n","\n","$$\n","\\mathcal{L}_{\\text{VAE}} = \\sum_{x\\sim \\mathcal{D}} \\mathcal{L}_{\\text{rec}}(x, \\hat{x}) - \\beta \\cdot D_{\\text{KL}}\\big(\\mathcal{N}(\\mu_q, \\sigma_q), \\mathcal{N}(0, I)\\big)\n","$$\n","\n","Here, $\\hat{x}$ is the reconstruction output of the decoder. In comparison to the auto-encoder objective, the VAE adds a regularizing term between the output of the encoder and a chosen prior distribution, effectively forcing the encoder output to not stray too far from the prior during training. As a result the decoder gets trained with samples that look pretty similar to samples from the prior, which will hopefully allow us to generate better images when using the VAE as a generative model and actually feeding it samples from the prior (as we have done for the AE before).\n","\n","The coefficient $\\beta$ is a scalar weighting factor that trades off between reconstruction and regularization objective. We will investigate the influence of this factor in out experiments below.\n","\n","If you need a refresher on VAEs you can check out this tutorial paper: https://arxiv.org/abs/1606.05908\n","\n","### Reparametrization Trick\n","\n","The sampling procedure inside the VAE's forward pass for obtaining a sample $z$ from the posterior distribution $q(z \\vert x)$, when implemented naively, is non-differentiable. However, since $q(z\\vert x)$ is parametrized with a Gaussian function, there is a simple trick to obtain a differentiable sampling operator, known as the _reparametrization trick_.\n","\n","Instead of directly sampling $z \\sim \\mathcal{N}(\\mu_q, \\sigma_q)$ we can \"separate\" the network's predictions and the random sampling by computing the sample as:\n","\n","$$\n","z = \\mu_q + \\sigma_q * \\epsilon , \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n","$$\n","\n","Note that in this equation, the sample $z$ is computed as a deterministic function of the network's predictions $\\mu_q$ and $\\sigma_q$ and therefore allows to propagate gradients through the sampling procedure.\n","\n","**Note**: While in the equations above the encoder network parametrizes the standard deviation $\\sigma_q$ of the Gaussian posterior distribution, in practice we usually parametrize the **logarithm of the standard deviation** $\\log \\sigma_q$ for numerical stability. Before sampling $z$ we will then exponentiate the network's output to obtain $\\sigma_q$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KSC7_Yy-n6-G"},"source":["## Defining the VAE Model [7pt]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qofk8oQ9tAxd"},"outputs":[],"source":["def kl_divergence(mu1, log_sigma1, mu2, log_sigma2):\n","  \"\"\"Computes KL[p||q] between two Gaussians defined by [mu, log_sigma].\"\"\"\n","  return (log_sigma2 - log_sigma1) + (torch.exp(log_sigma1) ** 2 + (mu1 - mu2) ** 2) \\\n","               / (2 * torch.exp(log_sigma2) ** 2) - 0.5\n","\n","# Prob1-4\n","class VAE(nn.Module):\n","  def __init__(self, nz, beta=1.0):\n","    super().__init__()\n","    self.beta = beta          # factor trading off between two loss components\n","    ####################################### TODO #######################################\n","    # Instantiate Encoder and Decoder.                                                 #\n","    # HINT: Remember that the encoder is now parametrizing a Gaussian distribution's   #\n","    #       mean and log_sigma, so the dimensionality of the output needs to           #\n","    #       double. The decoder works with an embedding sampled from this output.  #\n","    ####################################################################################\n","    \n","    #################################### END TODO ######################################\n","\n","  def forward(self, x):\n","    ####################################### TODO #######################################\n","    # Implement the forward pass of the VAE.                                           #\n","    # HINT: Your code should implement the following steps:                            #\n","    #          1. encode input x, split encoding into mean and log_sigma of Gaussian   #\n","    #          2. sample z from inferred posterior distribution using                  #\n","    #             reparametrization trick                                              #\n","    #          3. decode the sampled z to obtain the reconstructed image               #\n","    ####################################################################################\n","\n","    #################################### END TODO ######################################\n","\n","    return {'q': q, \n","            'rec': reconstruction}\n","\n","  def loss(self, x, outputs):\n","    ####################################### TODO #######################################\n","    # Implement the loss computation of the VAE.                                       #\n","    # HINT: Your code should implement the following steps:                            #\n","    #          1. compute the image reconstruction loss, similar to AE loss above      #\n","    #          2. compute the KL divergence loss between the inferred posterior        #\n","    #             distribution and a unit Gaussian prior; you can use the provided     #\n","    #             function above for computing the KL divergence between two Gaussians #\n","    #             parametrized by mean and log_sigma                                   #\n","    # HINT: Make sure to compute the KL divergence in the correct order since it is    #\n","    #       not symmetric!!  ie. KL(p, q) != KL(q, p)                                  #\n","    ####################################################################################\n","\n","    #################################### END TODO ######################################\n","\n","    # return weighted objective\n","    return rec_loss + self.beta * kl_loss, \\\n","           {'rec_loss': rec_loss, 'kl_loss': kl_loss}\n","    \n","  def reconstruct(self, x):\n","    \"\"\"Use mean of posterior estimate for visualization reconstruction.\"\"\"\n","    ####################################### TODO #######################################\n","    # This function is used for visualizing reconstructions of our VAE model. To       #\n","    # obtain the maximum likelihood estimate we bypass the sampling procedure of the   #\n","    # inferred latent and instead directly use the mean of the inferred posterior.     #\n","    # HINT: encode the input image and then decode the mean of the posterior to obtain #\n","    #       the reconstruction.                                                        #\n","    ####################################################################################\n","\n","\n","    #################################### END TODO ######################################\n","    return image\n"]},{"cell_type":"markdown","metadata":{"id":"EPCQZr-s_INx"},"source":["## Setting up the VAE Training Loop [4pt]\n","\n","Let's start training the VAE model! We will first verify our implementation by setting $\\beta = 0$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2StBTjj__HBU"},"outputs":[],"source":["# Prob1-5 VAE training loop\n","learning_rate = 1e-3\n","nz = 32\n","beta = 0\n","\n","####################################### TODO #######################################\n","epochs = 5      # recommended 5-20 epochs\n","#################################### END TODO ######################################\n","\n","# build VAE model\n","vae_model = VAE(nz, beta).to(device)    # transfer model to GPU if available\n","vae_model = vae_model.train()   # set model in train mode (eg batchnorm params get updated)\n","\n","# build optimizer and loss function\n","####################################### TODO #######################################\n","# Build the optimizer for the vae_model. We will again use the Adam optimizer with #\n","# the given learning rate and otherwise default parameters.                        #\n","####################################################################################\n","# same as AE\n","#################################### END TODO ######################################\n","\n","train_it = 0\n","rec_loss, kl_loss = [], []\n","print(f\"Running {epochs} epochs with {beta=}\")\n","for ep in range(epochs):\n","  print(\"Run Epoch {}\".format(ep))\n","  ####################################### TODO #######################################\n","  # Implement the main training loop for the VAE model.                              #\n","  # HINT: Your training loop should sample batches from the data loader, run the     #\n","  #       forward pass of the VAE, compute the loss, perform the backward pass and   #\n","  #       perform one gradient step with the optimizer.                              #\n","  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n","  # HINT: This time we will use the loss() function of our model for computing the   #\n","  #       training loss. It outputs the total training loss and a dict containing    #\n","  #       the breakdown of reconstruction and KL loss.                               #\n","  ####################################################################################\n","  for ...:\n","\n","\n","    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n","    if train_it % 100 == 0:\n","      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n","            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n","    train_it += 1\n","  #################################### END TODO ####################################\n","\n","print(\"Done!\")\n","\n","rec_loss_plotdata = [foo.detach().cpu() for foo in rec_loss]\n","kl_loss_plotdata = [foo.detach().cpu() for foo in kl_loss]\n","\n","# log the loss training curves\n","fig = plt.figure(figsize = (10, 5))   \n","ax1 = plt.subplot(121)\n","ax1.plot(rec_loss_plotdata)\n","ax1.title.set_text(\"Reconstruction Loss\")\n","ax2 = plt.subplot(122)\n","ax2.plot(kl_loss_plotdata)\n","ax2.title.set_text(\"KL Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YDK4A6WQPgUi"},"source":["Let's look at some reconstructions and decoded embedding samples!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KnFxjABPmQP"},"outputs":[],"source":["# visualize VAE reconstructions and samples from the generative model\n","vis_reconstruction(vae_model, randomize=True)\n","vis_samples(vae_model)"]},{"cell_type":"markdown","metadata":{"id":"BqFtLvsIEwRy"},"source":["## Tweaking the loss function $\\beta$ [2pt]\n","Prob1-6: Let's repeat the same experiment for $\\beta = 10$, a very high value for the coefficient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvQ23uV5PjXL"},"outputs":[],"source":["# VAE training loop\n","learning_rate = 1e-3\n","nz = 32\n","beta = 10\n","\n","####################################### TODO #######################################\n","epochs = 5      # recommended 5-20 epochs\n","#################################### END TODO ######################################\n","\n","# build VAE model\n","vae_model = VAE(nz, beta).to(device)    # transfer model to GPU if available\n","vae_model = vae_model.train()   # set model in train mode (eg batchnorm params get updated)\n","\n","# build optimizer and loss function\n","####################################### TODO #######################################\n","# Build the optimizer for the vae_model. We will again use the Adam optimizer with #\n","# the given learning rate and otherwise default parameters.                        #\n","####################################################################################\n","# same as AE\n","#################################### END TODO ######################################\n","\n","train_it = 0\n","rec_loss, kl_loss = [], []\n","print(f\"Running {epochs} epochs with {beta=}\")\n","for ep in range(epochs):\n","  print(\"Run Epoch {}\".format(ep))\n","  ####################################### TODO #######################################\n","  # Implement the main training loop for the VAE model.                              #\n","  # HINT: Your training loop should sample batches from the data loader, run the     #\n","  #       forward pass of the VAE, compute the loss, perform the backward pass and   #\n","  #       perform one gradient step with the optimizer.                              #\n","  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n","  # HINT: This time we will use the loss() function of our model for computing the   #\n","  #       training loss. It outputs the total training loss and a dict containing    #\n","  #       the breakdown of reconstruction and KL loss.                               #\n","  ####################################################################################\n","  for ...:\n","\n","\n","    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n","    if train_it % 100 == 0:\n","      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n","            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n","    train_it += 1\n","  #################################### END TODO ####################################\n","\n","print(\"Done!\")\n","\n","rec_loss_plotdata = [foo.detach().cpu() for foo in rec_loss]\n","kl_loss_plotdata = [foo.detach().cpu() for foo in kl_loss]\n","\n","# log the loss training curves\n","fig = plt.figure(figsize = (10, 5))   \n","ax1 = plt.subplot(121)\n","ax1.plot(rec_loss_plotdata)\n","ax1.title.set_text(\"Reconstruction Loss\")\n","ax2 = plt.subplot(122)\n","ax2.plot(kl_loss_plotdata)\n","ax2.title.set_text(\"KL Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"M_pdzfj--tUB"},"source":[">**Inline Question: What can you observe when setting $\\beta = 0$ and $\\beta = 10$? Explain your observations! [2pt]** \n",">(max 200 words) \n",">\n",">**Answer**:"]},{"cell_type":"markdown","metadata":{"id":"MpznmVGvGXjO"},"source":["## Obtaining the best $\\beta$-factor [5pt]\n","Prob 1-6 continued: Now we can start tuning the beta value to achieve a good result. First describe what a \"good result\" would look like (focus what you would expect for reconstructions and sample quality). \n","\n",">**Inline Question: Characterize what properties you would expect for reconstructions and samples of a well-tuned VAE! [3pt]**\n",">(max 200 words)\n",">\n",">**Answer**: \n",">\n","\n","Now that you know what outcome we would like to obtain, try to tune $\\beta$ to achieve this result. Logarithmic search in steps of 10x will be helpful, good results can be achieved after ~20 epochs of training. Training reconstructions should be high quality, test samples should be diverse, distinguishable numbers, most samples recognizable as numbers.\n","\n","**Answer: Tuned beta value _______ [2pt]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5d3Q5PD3QZs8"},"outputs":[],"source":["# Tuning for best beta\n","learning_rate = 1e-3\n","nz = 32\n","\n","####################################### TODO #######################################\n","epochs = 5      # recommended 5-20 epochs\n","beta = ... # Tune this for best results\n","#################################### END TODO ######################################\n","\n","# build VAE model\n","vae_model = VAE(nz, beta).to(device)    # transfer model to GPU if available\n","vae_model = vae_model.train()   # set model in train mode (eg batchnorm params get updated)\n","\n","# build optimizer and loss function\n","####################################### TODO #######################################\n","# Build the optimizer for the vae_model. We will again use the Adam optimizer with #\n","# the given learning rate and otherwise default parameters.                        #\n","####################################################################################\n","# same as AE\n","#################################### END TODO ######################################\n","\n","train_it = 0\n","rec_loss, kl_loss = [], []\n","print(f\"Running {epochs} epochs with {beta=}\")\n","for ep in range(epochs):\n","  print(\"Run Epoch {}\".format(ep))\n","  ####################################### TODO #######################################\n","  # Implement the main training loop for the VAE model.                              #\n","  # HINT: Your training loop should sample batches from the data loader, run the     #\n","  #       forward pass of the VAE, compute the loss, perform the backward pass and   #\n","  #       perform one gradient step with the optimizer.                              #\n","  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n","  # HINT: This time we will use the loss() function of our model for computing the   #\n","  #       training loss. It outputs the total training loss and a dict containing    #\n","  #       the breakdown of reconstruction and KL loss.                               #\n","  ####################################################################################\n","  for ...:\n","\n","\n","    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n","    if train_it % 100 == 0:\n","      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n","            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n","    train_it += 1\n","  #################################### END TODO ####################################\n","\n","print(\"Done!\")\n","\n","rec_loss_plotdata = [foo.detach().cpu() for foo in rec_loss]\n","kl_loss_plotdata = [foo.detach().cpu() for foo in kl_loss]\n","\n","# log the loss training curves\n","fig = plt.figure(figsize = (10, 5))   \n","ax1 = plt.subplot(121)\n","ax1.plot(rec_loss_plotdata)\n","ax1.title.set_text(\"Reconstruction Loss\")\n","ax2 = plt.subplot(122)\n","ax2.plot(kl_loss_plotdata)\n","ax2.title.set_text(\"KL Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VtU4-fHnAbYL"},"source":["# 4. Embedding Space Interpolation [3pt]\n","\n","As mentioned in the introduction, AEs and VAEs cannot only be used to generate images, but also to learn low-dimensional representations of their inputs. In this final section we will investigate the representations we learned with both models by **interpolating in embedding space** between different images. We will encode two images into their low-dimensional embedding representations, then interpolate these embeddings and reconstruct the result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VO_Dj-nAw6z"},"outputs":[],"source":["# Prob1-7\n","nz=32\n","\n","def get_image_with_label(target_label):\n","  \"\"\"Returns a random image from the training set with the requested digit.\"\"\"\n","  for img_batch, label_batch in mnist_data_loader:\n","    for img, label in zip(img_batch, label_batch):\n","      if label == target_label:\n","        return img.to(device)\n","\n","def interpolate_and_visualize(model, tag, start_img, end_img):\n","  \"\"\"Encodes images and performs interpolation. Displays decodings.\"\"\"\n","  model.eval()    # put model in eval mode to avoid updating batchnorm\n","\n","  # encode both images into embeddings (use posterior mean for interpolation)\n","  z_start = model.encoder(start_img[None].reshape(1,784))[..., :nz]\n","  z_end = model.encoder(end_img[None].reshape(1,784))[..., :nz]\n","\n","  # compute interpolated latents\n","  N_INTER_STEPS = 5\n","  z_inter = [z_start + i/N_INTER_STEPS * (z_end - z_start) for i in range(N_INTER_STEPS)]\n","\n","  # decode interpolated embeddings (as a single batch)\n","  img_inter = model.decoder(torch.cat(z_inter))\n","  img_inter = img_inter.reshape(-1, 28, 28)\n","\n","  # reshape result and display interpolation\n","  vis_imgs = torch.cat([start_img, img_inter, end_img]).reshape(-1,1,28,28)\n","  fig = plt.figure(figsize = (10, 10))   \n","  ax1 = plt.subplot(111)\n","  ax1.imshow(torchvision.utils.make_grid(vis_imgs, nrow=N_INTER_STEPS+2, pad_value=1.)\\\n","                  .data.cpu().numpy().transpose(1, 2, 0), cmap='gray')\n","  plt.title(tag)\n","  plt.show()\n","\n","\n","### Interpolation 1\n","START_LABEL = # ... TODO CHOOSE\n","END_LABEL = # ... TODO CHOOSE\n","# sample two training images with given labels\n","start_img = get_image_with_label(START_LABEL)\n","end_img = get_image_with_label(END_LABEL)\n","# visualize interpolations for AE and VAE models\n","interpolate_and_visualize(ae_model, \"Auto-Encoder\", start_img, end_img)\n","interpolate_and_visualize(vae_model, \"Variational Auto-Encoder\", start_img, end_img)\n","\n","### Interpolation 2\n","START_LABEL = # ... TODO CHOOSE\n","END_LABEL = # ... TODO CHOOSE\n","# sample two training images with given labels\n","start_img = get_image_with_label(START_LABEL)\n","end_img = get_image_with_label(END_LABEL)\n","# visualize interpolations for AE and VAE models\n","interpolate_and_visualize(ae_model, \"Auto-Encoder\", start_img, end_img)\n","interpolate_and_visualize(vae_model, \"Variational Auto-Encoder\", start_img, end_img)\n"]},{"cell_type":"markdown","metadata":{"id":"YdQKHspOF_5Q"},"source":["Repeat the experiment for different start / end labels and different samples. Describe your observations.\n","\n",">**Prob1-7 continued: Inline Question: Repeat the interpolation experiment with different start / end labels and multiple samples. Describe your observations! [2 pt]**\n",">  1. How do AE and VAE embedding space interpolations differ?\n",">  2. How do you expect these differences to affect the usefulness of the learned representation for downstream learning?\n",">(max 300 words)\n",">\n",">**Answer**:\n",">"]},{"cell_type":"markdown","metadata":{"id":"o4AuOGRUXwwG"},"source":["# 5. Conditional VAE\n","Let us now try a Conditional VAE\n","Now we will try to create a [Conditional VAE](https://proceedings.neurips.cc/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf), where we can condition the encoder and decoder of the VAE on the label `c`. "]},{"cell_type":"markdown","metadata":{"id":"DkAuIglQXwwG"},"source":["## Defining the conditional Encoder, Decoder, and VAE models [5 pt]\n","\n","Prob1-8. We create a separate encoder and decoder class that take in an additional argument `c` in their forward pass, and then build our CVAE model on top of it. Note that the encoder and decoder just need to append `c` to the standard inputs to these modules."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33EJfCfBXwwG"},"outputs":[],"source":["def idx2onehot(idx, n):\n","    \"\"\"Converts a batch of indices to a one-hot representation.\"\"\"\n","    assert torch.max(idx).item() < n\n","    if idx.dim() == 1:\n","        idx = idx.unsqueeze(1)\n","    onehot = torch.zeros(idx.size(0), n).to(idx.device)\n","    onehot.scatter_(1, idx, 1)\n","\n","    return onehot\n","\n","# Let's define encoder and decoder networks\n","\n","class CVAEEncoder(nn.Module):\n","  def __init__(self, nz, input_size, conditional, num_labels):\n","    super().__init__()\n","    self.input_size = input_size + num_labels if conditional else input_size\n","    self.num_labels = num_labels\n","    self.conditional = conditional\n","\n","    ################################# TODO #########################################\n","    # Create the network architecture using a nn.Sequential module wrapper.        #\n","    # Encoder Architecture:                                                        #\n","    # - input_size -> 256                                                          #\n","    # - ReLU                                                                       #\n","    # - 256 -> 64                                                                  #\n","    # - ReLU                                                                       #\n","    # - 64 -> nz                                                                   #\n","    # HINT: Verify the shapes of intermediate layers by running partial networks   #\n","    #        (with the next notebook cell) and visualizing the output shapes.      #\n","    ################################################################################\n","\n","    ################################ END TODO ######################################\n","\n","  def forward(self, x, c=None):\n","    ################################# TODO #########################################\n","    # If using conditional VAE, concatenate x and a onehot version of c to create  #\n","    # the full input. Use function idx2onehot above.                               #\n","    ################################################################################\n","\n","    ################################################################################\n","    return self.net(x)\n","\n","\n","class CVAEDecoder(nn.Module):\n","  def __init__(self, nz, output_size, conditional, num_labels):\n","    super().__init__()\n","    self.output_size = output_size\n","    self.conditional = conditional\n","    self.num_labels = num_labels\n","    if self.conditional:\n","        nz = nz + num_labels\n","    ################################# TODO #########################################\n","    # Create the network architecture using a nn.Sequential module wrapper.        #\n","    # Decoder Architecture (mirrors encoder architecture):                         #\n","    # - nz -> 64                                                                   #\n","    # - ReLU                                                                       #\n","    # - 64 -> 256                                                                  #\n","    # - ReLU                                                                       #\n","    # - 256 -> output_size                                                         #\n","    ################################################################################\n","\n","    ################################ END TODO #######################################\n","\n","  def forward(self, z, c=None):\n","    ################################# TODO #########################################\n","    # If using conditional VAE, concatenate z and a onehot version of c to create  #\n","    # the full embedding. Use function idx2onehot above.                           #\n","    ################################################################################\n","\n","    ################################ END TODO #######################################\n","\n","    return self.net(z).reshape(-1, 1, self.output_size)\n","\n","\n","class CVAE(nn.Module):\n","    def __init__(self, nz, beta=1.0, conditional=False, num_labels=0):\n","        super().__init__()\n","        if conditional:\n","            assert num_labels > 0\n","        self.beta = beta\n","        self.encoder = CVAEEncoder(2*nz, input_size=in_size, conditional=conditional, num_labels=num_labels)\n","        self.decoder = CVAEDecoder(nz, output_size=out_size, conditional=conditional, num_labels=num_labels)\n","\n","    def forward(self, x, c=None):\n","        if x.dim() > 2:\n","            x = x.view(-1, 28*28)\n","\n","        q = self.encoder(x,c)\n","        mu, log_sigma = torch.chunk(q, 2, dim=-1)\n","\n","        # sample latent variable z with reparametrization\n","        eps = torch.normal(mean=torch.zeros_like(mu), std=torch.ones_like(log_sigma))\n","        # eps = torch.randn_like(mu) # Alternatively use this\n","        z = mu + eps * torch.exp(log_sigma)\n","\n","        # compute reconstruction\n","        reconstruction = self.decoder(z, c)\n","\n","        return {'q': q, 'rec': reconstruction, 'c': c}\n","\n","    def loss(self, x, outputs):\n","        ####################################### TODO #######################################\n","        # Implement the loss computation of the VAE.                                       #\n","        # HINT: Your code should implement the following steps:                            #\n","        #          1. compute the image reconstruction loss, similar to AE loss above      #\n","        #          2. compute the KL divergence loss between the inferred posterior        #\n","        #             distribution and a unit Gaussian prior; you can use the provided     #\n","        #             function above for computing the KL divergence between two Gaussians #\n","        #             parametrized by mean and log_sigma                                   #\n","        # HINT: Make sure to compute the KL divergence in the correct order since it is    #\n","        #       not symmetric!!  ie. KL(p, q) != KL(q, p)                                  #\n","        ####################################################################################\n","\n","        #################################### END TODO ######################################\n","\n","        # return weighted objective\n","        return rec_loss + self.beta * kl_loss, \\\n","            {'rec_loss': rec_loss, 'kl_loss': kl_loss}\n","    \n","    def reconstruct(self, x, c=None):\n","        \"\"\"Use mean of posterior estimate for visualization reconstruction.\"\"\"\n","        ####################################### TODO #######################################\n","        # This function is used for visualizing reconstructions of our VAE model. To       #\n","        # obtain the maximum likelihood estimate we bypass the sampling procedure of the   #\n","        # inferred latent and instead directly use the mean of the inferred posterior.     #\n","        # HINT: encode the input image and then decode the mean of the posterior to obtain #\n","        #       the reconstruction.                                                        #\n","        ####################################################################################\n","\n","        #################################### END TODO ######################################\n","        return image"]},{"cell_type":"markdown","metadata":{"id":"iAPxvyUsXwwH"},"source":["## Setting up the CVAE Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwpfvKaqXwwH"},"outputs":[],"source":["learning_rate = 1e-3\n","nz = 32\n","\n","####################################### TODO #######################################\n","# Tune the beta parameter to obtain good training results. However, for the    #\n","# initial experiments leave beta = 0 in order to verify our implementation.        #\n","####################################################################################\n","epochs = 5 # works with fewer epochs than AE, VAE. we only test conditional samples.\n","beta = 0\n","#################################### END TODO ######################################\n","\n","# build CVAE model\n","conditional = True\n","cvae_model = CVAE(nz, beta, conditional=conditional, num_labels=10).to(device)    # transfer model to GPU if available\n","cvae_model = cvae_model.train()   # set model in train mode (eg batchnorm params get updated)\n","\n","# build optimizer and loss function\n","####################################### TODO #######################################\n","# Build the optimizer for the cvae_model. We will again use the Adam optimizer with #\n","# the given learning rate and otherwise default parameters.                        #\n","####################################################################################\n","# same as AE\n","#################################### END TODO ######################################\n","\n","train_it = 0\n","rec_loss, kl_loss = [], []\n","print(f\"Running {epochs} epochs with {beta=}\")\n","for ep in range(epochs):\n","  print(f\"Run Epoch {ep}\")\n","  ####################################### TODO #######################################\n","  # Implement the main training loop for the model.                                  #\n","  # If using conditional VAE, remember to pass the conditional variable c to the     #\n","  # forward pass                                                                     #\n","  # HINT: Your training loop should sample batches from the data loader, run the     #\n","  #       forward pass of the model, compute the loss, perform the backward pass and #\n","  #       perform one gradient step with the optimizer.                              #\n","  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n","  # HINT: As before, we will use the loss() function of our model for computing the  #\n","  #       training loss. It outputs the total training loss and a dict containing    #\n","  #       the breakdown of reconstruction and KL loss.                               #\n","  ####################################################################################\n","  for ...:\n","\n","    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n","    if train_it % 100 == 0:\n","      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n","            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n","    train_it += 1\n","  #################################### END TODO ####################################\n","\n","print(\"Done!\")\n","\n","rec_loss_plotdata = [foo.detach().cpu() for foo in rec_loss]\n","kl_loss_plotdata = [foo.detach().cpu() for foo in kl_loss]\n","\n","# log the loss training curves\n","fig = plt.figure(figsize = (10, 5))\n","ax1 = plt.subplot(121)\n","ax1.plot(rec_loss_plotdata)\n","ax1.title.set_text(\"Reconstruction Loss\")\n","ax2 = plt.subplot(122)\n","ax2.plot(kl_loss_plotdata)\n","ax2.title.set_text(\"KL Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ePn5sFKCXwwH"},"source":["### Verifying conditional samples from CVAE [6 pt]\n","Now let us generate samples from the trained model, conditioned on all the labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AQOCmLCXwwI"},"outputs":[],"source":["# Prob1-9\n","if conditional:\n","    c = torch.arange(0, 10).long().unsqueeze(1).to(device)\n","    z = torch.randn([10, nz]).to(device)\n","    x = cvae_model.decoder(z, c=c)\n","else:\n","    z = torch.randn([10, nz]).to(device)\n","    x = cvae_model.decoder(z)\n","\n","plt.figure()\n","plt.figure(figsize=(5, 10))\n","for p in range(10):\n","    plt.subplot(5, 2, p+1)\n","    if conditional:\n","        plt.text(\n","            0, 0, \"c={:d}\".format(c[p].item()), color='black',\n","            backgroundcolor='white', fontsize=8)\n","    plt.imshow(x[p].view(28, 28).cpu().data.numpy(), cmap='gray')\n","    plt.axis('off')"]},{"cell_type":"markdown","metadata":{"id":"ytPPsGz2vBx0"},"source":["# Submission Instructions\n","\n","You need to submit this jupyter notebook and a PDF. See Piazza for detailed submission instructions."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"vaemnist","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"30a36b704feb31e8d62934e0f6235a63429541b16e7656344ff7eed2d46d043a"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"00ccb5258094474484c970e4b698d0e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"074608e2e6c646d8be29bee287d4eecd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08d643e9e1ed4f5f8a4b938513eb74b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8f09be5e62e446cb81ea9ba618a2b05","placeholder":"","style":"IPY_MODEL_cc44f8e2f9d94dac82132168dfa615cb","value":" 28881/28881 [00:00&lt;00:00, 807119.31it/s]"}},"0c837a309a8749e9acb998d26b53eb03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1069f776c52b4b53be617bbf368fcc5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16c9106fb10d43c28815c73e0a960c32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dbc78cf70d64fc094c758bc5c06b8bb","placeholder":"","style":"IPY_MODEL_d80c21ce156748ddae0ff0bcccaccf41","value":"100%"}},"251a7d7cccc243a7a9afb379f40b6abc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25c51dbd460048a0ade87d231b919eba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2cb89c5dca2444aa899dc73dc3e281f5","IPY_MODEL_e117f794f56b4c45894647d748a0f6f4","IPY_MODEL_61ede69ade0243fb8b9fbcdf8ccabf61"],"layout":"IPY_MODEL_ec99aa403334405184322466cdef6ded"}},"2cb89c5dca2444aa899dc73dc3e281f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_074608e2e6c646d8be29bee287d4eecd","placeholder":"","style":"IPY_MODEL_e41aa50f2c594af084fa1af6b9d9a4b3","value":"100%"}},"316e858872234de9a3300a355a32df5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"430e2502449c47a5a23f47eaf088a894":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c207577fbf14d098de29220c1a200d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d09b2b8b271b46788d00e6a1442699de","IPY_MODEL_f321c1db62ad4645916b08460fd6b40a","IPY_MODEL_08d643e9e1ed4f5f8a4b938513eb74b2"],"layout":"IPY_MODEL_1069f776c52b4b53be617bbf368fcc5e"}},"53b496dc0326451994983d34e32eabcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c84c00ce102843c1a5567338fa433747","placeholder":"","style":"IPY_MODEL_c1e61faf117747e49ba18e9e8c07c359","value":" 4542/4542 [00:00&lt;00:00, 84286.54it/s]"}},"55b2698f8bab4a2e9b40bad1ac28580c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5795e5119e744f3b95af1061b694019d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d868cebd70a4bcf9377ffb62f9adb86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16c9106fb10d43c28815c73e0a960c32","IPY_MODEL_d885bf767bc64103a16be48fce9cb9a9","IPY_MODEL_7aab706d77ee449881a0a1e199a8e845"],"layout":"IPY_MODEL_251a7d7cccc243a7a9afb379f40b6abc"}},"5dbc78cf70d64fc094c758bc5c06b8bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61ede69ade0243fb8b9fbcdf8ccabf61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6abdefd833a4850a6d4fc5532a0a31f","placeholder":"","style":"IPY_MODEL_ec1a9588948e46bb85f9a2cfc2afe690","value":" 9912422/9912422 [00:00&lt;00:00, 166253633.90it/s]"}},"669a51ec535b4841b19bafc1cf734534":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79a7c29698064a238d27747ed85b3442":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7aab706d77ee449881a0a1e199a8e845":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00ccb5258094474484c970e4b698d0e9","placeholder":"","style":"IPY_MODEL_669a51ec535b4841b19bafc1cf734534","value":" 1648877/1648877 [00:00&lt;00:00, 28279261.67it/s]"}},"a11343c41bcd4a71912a80830d5bd5b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1e61faf117747e49ba18e9e8c07c359":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c84c00ce102843c1a5567338fa433747":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc44f8e2f9d94dac82132168dfa615cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d09b2b8b271b46788d00e6a1442699de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5795e5119e744f3b95af1061b694019d","placeholder":"","style":"IPY_MODEL_e777a7f4889840be88dffb206d851bf4","value":"100%"}},"d1396522b53047149210c329510307fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c837a309a8749e9acb998d26b53eb03","max":4542,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79a7c29698064a238d27747ed85b3442","value":4542}},"d4caee53c3e049c7ba72e00ba03d358a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55b2698f8bab4a2e9b40bad1ac28580c","placeholder":"","style":"IPY_MODEL_eee3ae88d3db4a05b23b7f6e14e176cb","value":"100%"}},"d6abdefd833a4850a6d4fc5532a0a31f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d80c21ce156748ddae0ff0bcccaccf41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d885bf767bc64103a16be48fce9cb9a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a11343c41bcd4a71912a80830d5bd5b6","max":1648877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f5ee2f461d0c41e8b03f2450ccb0845b","value":1648877}},"dd354536c91840b8af906237e2ad1540":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dece586b88a74ffbb5dba17157cac40c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e117f794f56b4c45894647d748a0f6f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd354536c91840b8af906237e2ad1540","max":9912422,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebf2a93e8e29426c9a944ff9f6e53a2a","value":9912422}},"e41aa50f2c594af084fa1af6b9d9a4b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e777a7f4889840be88dffb206d851bf4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebf2a93e8e29426c9a944ff9f6e53a2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec1a9588948e46bb85f9a2cfc2afe690":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec99aa403334405184322466cdef6ded":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eee3ae88d3db4a05b23b7f6e14e176cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f321c1db62ad4645916b08460fd6b40a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_430e2502449c47a5a23f47eaf088a894","max":28881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_316e858872234de9a3300a355a32df5d","value":28881}},"f39a85e3cf3f4fdfb8141c16c90f5cee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d4caee53c3e049c7ba72e00ba03d358a","IPY_MODEL_d1396522b53047149210c329510307fa","IPY_MODEL_53b496dc0326451994983d34e32eabcf"],"layout":"IPY_MODEL_dece586b88a74ffbb5dba17157cac40c"}},"f5ee2f461d0c41e8b03f2450ccb0845b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8f09be5e62e446cb81ea9ba618a2b05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
